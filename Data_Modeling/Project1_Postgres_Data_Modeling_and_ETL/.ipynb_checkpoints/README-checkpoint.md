---
# Project: Data Modeling with Postgres

Purpose and Implementation of the this project contains
- Data modeling with Postgres
- Define fact and dimension tables for a star schema
- Build an ETL pipeline using Python (transfer data from files to  tables in Postgres )

## Introduction

A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

Developer is responsible for creating a data modeling with Postgres and build an ETL pipeline using Python for the analysis.

### Data Sets
- **<em>Song dataset</em>**: <br> JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.<br>All files are nested in subdirectories under */data/song_data*.<br>Below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.
```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

- **<em>Log dataset</em>**: <br>log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.<br>The log files in the dataset you'll be working with are partitioned by year and month.<br>All files are nested in subdirectories under */data/log_data*. <br>Below is an example record in log files:

```
{"artist":"Des'ree","auth":"Logged In","firstName":"Kaylee","gender":"F","itemInSession":1,"lastName":"Summers","length":246.30812,"level":"free","location":"Phoenix-Mesa-Scottsdale, AZ","method":"PUT","page":"NextSong","registration":1540344794796.0,"sessionId":139,"song":"You Gotta Be","status":200,"ts":1541106106796,"userAgent":"\"Mozilla\/5.0 (Windows NT 6.1; WOW64) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/35.0.1916.153 Safari\/537.36\"","userId":"8"}
```

## Database Schema
The schema used for this exercise is the Star Schema:
There is one main fact table containing all the measures associated to each event (user song plays),
and 4 dimentional tables, each with a primary key that is being referenced from the fact table.

On why to use a relational database for this case:
- The data types are structured (we know before-hand the sctructure of the jsons we need to analyze, and where and how to extract and transform each field)
- The amount of data we need to analyze is not big enough to require big data related solutions.
- Ability to use SQL that is more than enough for this kind of analysis
- Data needed to answer business questions can be modeled using simple ERD models
- We need to use JOINS for this scenario

#### Dimension Tables
**<em>users</em>** - users in the app
- user_id (varchar) PRIMARY KEY: (PK) Id of user
- first_name (varchar) NOT NULL: first Name of the user
- last_name (varchar) NOT NULL: last Name of the user
- gender (varchar): Gender of the user (M, F)
- level (varchar): User level  (free, paid)

**<em>artists</em>** - artists in music database
- artist_id (varchar) PRIMARY KEY: (PK) Id of Artist
- name (varchar) NOT NULL: Name of Artist
- location (varchar): City Name of location of Artist
- lattitude (FLOAT): Lattitude of location of Artist
- longitude (FLOAT): Longitude of location of Artist

**<em>songs</em>** - songs in music database
- song_id varchar PRIMARY KEY: (PK) Id of the song
- title varchar NOT NULL: title of the song
- artist_id varchar NOT NULL: Artist's Id of the song
- year int: published year of the song
- duration float: duraiton of the song in milliseconds

**<em>time</em>** - timestamps of records in songplays broken down into specific units
- start_time timestamp PRIMARY KEY: (PK) Timestamp of the record
- hour int: Hour value of start_time
- day int: Day value of start_time
- week int: Week value of start_time
- month int: Month value of start_time
- year int: Year value of start_time
- weekday varchar: Name of week day of start_time


#### Fact Table
**<em>songplays</em>** - records in log data associated with song plays i.e. records with page NextSong
- songplay_id int PRIMARY KEY: PK of each song user played
- start_time timestamp: Start Time of the song play
- user_id varchar: ID of user
- level varchar: User level (free, paid)
- song_id varchar: ID of Song played
- artist_id varchar: ID of Artist of Song played
- session_id varchar: ID of the user Session
- location varchar: User's location
- user_agent varchar: The Agent user used for Sparkify

## Project structure

Files used on the project:
1. **data** folder nested at the home of the project, where all needed jsons reside.
2. **test.ipynb** displays the first few rows of each table to let you check your database.
3. **create_tables.py** drops and creates your tables. You run this file to reset your tables before each time you run your ETL scripts.
4. **etl.ipynb** reads and processes a single file from <em>**song_data**</em> and <em>**log_data**</em> and loads the data into your tables. This notebook contains detailed instructions on the ETL process for each of the tables.
5. **etl.py** reads and processes ALL files from <em>**song_data**</em> and <em>**log_data**</em> and loads them into Postgres tables.
6. **sql_queries.py** contains all your sql queries, and is imported into the last three files above.
7. **README.md** provides discussion on the project.

### Project Steps

1. Write **CREATE** statements in **sql_queries**.py to create each table.

2. Write **DROP** statements in **sql_queries.py** to drop each table if it exists.

3. Run **create_tables.py** in console to create your database and tables.
```
python create_tables.py
```

4. Run **test.ipynb** to confirm the creation of your tables with the correct columns. Make sure to click "Restart kernel" to close the connection to the database after running this notebook.

5. Followed the instructions and completed **etl.ipynb** Notebook to develop the ETL process .

6. Use what you've completed in this notebook to implement **etl.py**.

7. Run **etl.py** in console and verify results:
```
python etl.py
```

## ETL pipeline
1. **(ONLY the first time)** on the **create_tables.py**, we create the table schema (Fact Table and Dimension Tables) in sparkify database.

2. on the **etl.py**, we connect to the sparkify database and process song dataset files under the subfolder /data/song_data and insert the data into the dimension tables.

```
song_data = [song_id, title, artist_id, year, duration]
```
```
artist_data = [artist_id, artist_name, artist_location, artist_longitude, artist_latitude]
```

3. on the **etl.py**, we connect to the sparkify database and process log dataset files under the subfolder /data/log_data and insert the data into the dimension tables. <br>3 main transform steps here:
  - a. convert ts column and insert into time dimension table
  - b. filter the data by page = 'NextSong' to get the song playlist
  - c. Lookup song and artist id from their tables by song name, artist name and song duration that we have on our song play data. The query as below:
    ```
    song_select = ("""
        SELECT song_id, songs.artist_id
          FROM songs JOIN artists on songs.artist_id = artists.artist_id
         WHERE title = %s
           AND name = %s
           AND duration = %s
    """)
    ```
4. Last step in the **etl.py**, insert the related data into our songplays fact table.

5. You can check the data via the last command in test.jpynb
```
%sql SELECT * FROM songplays LIMIT 2;
```
