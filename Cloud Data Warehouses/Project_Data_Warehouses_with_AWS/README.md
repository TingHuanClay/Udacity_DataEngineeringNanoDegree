---
# Project: Data Warehouses

Purpose and Implementation of the this project contains
- Create IAM role and Redshift cluster using IaC (Infratructure as Code)
- Define staging tables according to the data set in s3 bucket
- Define fact table and dimension tables for a star schema
- Build an ETL pipeline using Python (copy data from files in s3 to  staging tables in redshift and transfer data from to the fact and dimension tables.)
- Define analytic query to vaeify ETL result.

## Introduction

A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

As their data engineer, you are tasked with building an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights in what songs their users are listening to. You'll be able to test your database and ETL pipeline by running queries given to you by the analytics team from Sparkify and compare your results with their expected results.

Developer is responsible for knowledge on data warehouses and AWS to build an ETL pipeline for a database hosted on Redshift. To complete the project, you will need to load data from S3 to staging tables on Redshift and execute SQL statements that create the analytics tables from these staging tables.

### Data Sets
Developer will be working with two datasets that reside in S3. Here are the S3 links for each:
<br>Log data json path:s3://udacity-dend/log_json_path.json
- **<em>Song dataset</em>**: s3://udacity-dend/song_data<br>
JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.<br>All files are nested in subdirectories under */data/song_data*.<br>Below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.
```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

- **<em>Log dataset</em>**: s3://udacity-dend/log_data<br>
log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.<br>The log files in the dataset you'll be working with are partitioned by year and month.<br>All files are nested in subdirectories under */data/log_data*. <br>Below is an example record in log files:

```
{"artist":"Des'ree","auth":"Logged In","firstName":"Kaylee","gender":"F","itemInSession":1,"lastName":"Summers","length":246.30812,"level":"free","location":"Phoenix-Mesa-Scottsdale, AZ","method":"PUT","page":"NextSong","registration":1540344794796.0,"sessionId":139,"song":"You Gotta Be","status":200,"ts":1541106106796,"userAgent":"\"Mozilla\/5.0 (Windows NT 6.1; WOW64) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/35.0.1916.153 Safari\/537.36\"","userId":"8"}
```

## Database Schema
The schema used for this exercise is the Star Schema:
There are 2 staging tables would used to copy data from s3 bucket
There is one main fact table containing all the measures associated to each event (user song plays),
and 4 dimension tables, each with a primary key that is being referenced from the fact table.

On why to use a relational database for this case:
- The data types are structured (we know before-hand the sctructure of the jsons we need to analyze, and where and how to extract and transform each field)
- The amount of data we need to analyze is not big enough to require big data related solutions.
- Ability to use SQL that is more than enough for this kind of analysis
- Data needed to answer business questions can be modeled using simple ERD models
- We need to use JOINS for this scenario


#### Staging Table
**<em>staging_events</em>** - records in log data associated with song plays i.e. records with page NextSong
- artist              VARCHAR:
- auth                VARCHAR:
- firstName           VARCHAR:
- gender              VARCHAR:
- itemInSession       INTEGER:
- lastName            VARCHAR:
- length              FLOAT:
- level               VARCHAR:
- location            VARCHAR:
- method              VARCHAR:
- page                VARCHAR:
- registration        FLOAT:
- sessionId           INTEGER:
- song                VARCHAR:
- status              INTEGER:
- ts                  TIMESTAMP:
- userAgent           VARCHAR:
- userId              INTEGER:

**<em>staging_songs</em>** - records in log data associated with song plays i.e. records with page NextSong
- num_songs           INTEGER:
- artist_id           VARCHAR:
- artist_latitude     FLOAT:
- artist_longitude    FLOAT:
- artist_location     VARCHAR:
- artist_name         VARCHAR:
- song_id             VARCHAR:
- title               VARCHAR:
- duration            FLOAT:
- year                INTEGER:


#### Fact Table
**<em>songplays</em>** - records in log data associated with song plays i.e. records with page NextSong
- songplay_id INTEGER     IDENTITY(0,1)   PRIMARY KEY: PK of each song user played, automatically created by serialbumber
- start_time  TIMESTAMP   NOT NULL SORTKEY DISTKEY: Start Time of the song play
- user_id     VARCHAR     NOT NULL: ID of user
- level       VARCHAR: User level (free, paid)
- song_id     VARCHAR     NOT NULL: ID of Song played
- artist_id   VARCHAR     NOT NULL: ID of Artist of Song played
- session_id  VARCHAR: ID of the user Session
- location    VARCHAR: User's location
- user_agent  VARCHAR: The Agent user used for Sparkify

#### Dimension Tables
**<em>users</em>** - users in the app
- user_id     VARCHAR     PRIMARY KEY SORTKEY: (PK) Id of user
- first_name  VARCHAR     NOT NULL: first Name of the user
- last_name   VARCHAR     NOT NULL: last Name of the user
- gender      VARCHAR: Gender of the user (M, F)
- level       VARCHAR: User level  (free, paid)

**<em>artists</em>** - artists in music database
- artist_id   VARCHAR     PRIMARY KEY SORTKEY: (PK) Id of Artist
- name        VARCHAR     NOT NULL: Name of Artist
- location    VARCHAR: City Name of location of Artist
- latitude    FLOAT: Lattitude of location of Artist
- longitude   FLOAT: Longitude of location of Artist

**<em>songs</em>** - songs in music database
- song_id     VARCHAR     PRIMARY KEY SORTKEY: (PK) Id of the song
- title       VARCHAR     NOT NULL: title of the song
- artist_id   VARCHAR     NOT NULL: Artist's Id of the song
- year        INTEGER: published year of the song
- duration    FLOAT: duraiton of the song in milliseconds

**<em>time</em>** - timestamps of records in songplays broken down into specific units
- start_time  TIMESTAMP   PRIMARY KEY DISTKEY SORTKEY: (PK) Timestamp of the record
- hour        INTEGER: Hour value of start_time
- day         INTEGER: Day value of start_time
- week        INTEGER: Week value of start_time
- month       INTEGER: Month value of start_time
- year        INTEGER: Year value of start_time
- weekday     VARCHAR: Name of week day of start_time

## Project structure

Files used on the project:
1. **check_etl_result.py** count data from table in Redshift and check the result in the end.
2. **create_clusters_and_iamrole.py**
3. **create_tables.py** drops and creates your tables. You run this file to reset your tables before each time you run your ETL scripts.
4. **delete_clusters_and_iamrole.py** delete the redshift cluster and IAM role created via **create_clusters_and_iamrole.py**.
5. **dwh.cfg** config the essential AWS setting project required.
6. **etl.py** reads and processes ALL files <em>**song_data**</em> and <em>**log_data**</em> from S3 bucket and loads them into staging tables on Redshift cluster, then extract data from staging tables and load to fact table and deimension tables.
7. **README.md** provides discussion on the project.
8. **sql_queries.py** contains all your sql queries, and is imported into the files above (**create_tables.py**, **etl.py** and **check_etl_result.py**.)

### How to Run

1. Prepare system according to system requirement

| library | version |
| ------ | ---- |
| `awscli` | `1.16.140` |
| `boto3` | `1.9.164` |
| `botocore` | `1.12.164` |
| `pandas` | `0.23.4` |
| `psycopg2` | `2.7.7` |
| `psycopg2-binary` | `2.8.2` |


2. Fill in essential configuration in **dwh.cfg** file
```
[CLUSTER]
HOST       	=
DB_NAME     =
DB_USER     =
DB_PASSWORD =
DB_PORT     = 5439

[IAM_ROLE]
ARN=

[AWS]
KEY=
SECRET=

[S3]
LOG_DATA='s3://udacity-dend/log_data'
LOG_JSONPATH='s3://udacity-dend/log_json_path.json'
SONG_DATA='s3://udacity-dend/song_data'

[DWH]
DWH_CLUSTER_TYPE       = multi-node
DWH_NUM_NODES          = 4
DWH_NODE_TYPE          = dc2.large
DWH_CLUSTER_IDENTIFIER =
DWH_DB                 =
DWH_DB_USER            =
DWH_DB_PASSWORD        =
DWH_PORT               = 5439
DWH_IAM_ROLE_NAME      =
DWH_ENDPOINT           =

```

3. Run **create_clusters_and_iamrole.py** in console to create IAM role and Redshift cluster<br>
  `$ python create_clusters_and_iamrole.py`
  <br><br>
  ![DER](https://raw.githubusercontent.com/TingHuanClay/Udacity_DataEngineeringNanoDegree/master/Cloud%20Data%20Warehouses/Project_Data_Warehouses_with_AWS/img/create_cluster_and_iamrole.png)


4. **Do NOT run this script until previous step is finished. Maku sure you see the message from console as the snapshot above**<br>Run **create_tables.py** in console to drop and create tables (staging, fact and dimension tables)<br>
  `$ python create_tables.py`
  <br><br>
  ![DER](https://raw.githubusercontent.com/TingHuanClay/Udacity_DataEngineeringNanoDegree/master/Cloud%20Data%20Warehouses/Project_Data_Warehouses_with_AWS/img/create_table.png)


5. Run **etl.py** in console for copying data from s3 to staging tables and inserting data into fact table and dimension tables<br>
  `$ python etl.py`
  <br><br>
  ![DER](https://raw.githubusercontent.com/TingHuanClay/Udacity_DataEngineeringNanoDegree/master/Cloud%20Data%20Warehouses/Project_Data_Warehouses_with_AWS/img/etl.png)

6. Run **check_etl_result.py** in console to validate result of etl process<br>
  `$ python check_etl_result.py`
  <br><br>
  ![DER](https://raw.githubusercontent.com/TingHuanClay/Udacity_DataEngineeringNanoDegree/master/Cloud%20Data%20Warehouses/Project_Data_Warehouses_with_AWS/img/check_etl_result.png)

7.  Run **delete_clusters_and_iamrole.py** in console to delete the redshift cluster and IAM role.<br>
  `$ python delete_clusters_and_iamrole.py`
  <br><br>
  ![DER](https://raw.githubusercontent.com/TingHuanClay/Udacity_DataEngineeringNanoDegree/master/Cloud%20Data%20Warehouses/Project_Data_Warehouses_with_AWS/img/delete_cluster_and_iamrole.png)
